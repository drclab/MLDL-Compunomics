{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Say our input is a 32-pixel by 32-pixel grayscale image where every pixel in the grayscale image is represented by a value between 0 to 255. Here, 0 indicates black, 255 represents indicates white, and any values between 0 and 255 indicate various shades of gray. Since the grayscale image has only one channel, the image can be represented as 32 x 32 x 1 = 1024, and consequently has 1,024 nodes in the input layer of the FNN. Suppose our next layer (hidden layer) has 100 nodes, and since this should be fully connected to the previous layer, we will have 1,024 x 100 = 102,400 weights for the first 2 layers (input and the first hidden layer). Since we know by now that a complex problem such as this requires multiple hidden layers in the FNN to map the inputs and the outputs in the training data to generate an accurate model, as such, we now have a problem with too many parameters in the FNN, which makes the training process very complex because of the increased dimension space. In addition, it makes the learning process slower, uses more resources, and increases the chances of overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If we are to use the color images, the problem is further compounded because the color images have 3 channels (red, green, and blue) where each color is represented by a channel, and so in total there are 3 values and 32 x 32 x 3 = 3,072 values. Correspondingly, the number of weights for such an image is 3,072 x 100 = 3,072,000. FNNs cannot scale to handle these kinds of images, so there needs to be a better scalable architecture. Another limitation of FNNs with image data is that the 2D image is converted to a 1D flattened vector, and so the spatial relationship of the different pixels is completely lost. So, there needs to be a better NN architecture that can keep spatial relationships and process unstructured data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs are a special type of NN architecture that is widely applied for unstructured data such as images, audio, video, text, and so on. CNNs are great networks for analyzing data with spatial dependencies such as images, audio, DNA sequences, and so on. They work well on DNA sequence data. The main applications of CNNs include image classification, NLP, signal processing, and speech recognition. CNNs have a series of convolutional layers, which allows them to automatically extract hierarchical patterns in the data. CNNs are currently being used in genomics tasks where local patterns are very important to the outcome—for example, the detection of conserved motifs to identify blocks of genes in a DNA sequence or binding sites of a protein such as a transcription factor (TF). Let’s dive deep into the wonderful world of CNNs in the following sections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_05_002.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A convolutional layer consists of multiple filters (also called kernels) that are nothing but the matrix of weights—for example, a filter of size 9 (3 by 3) that is initialized with values randomly between 0 to 10."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next, convolution is done by placing this filter at the top-left corner in the case of the image and taking the dot product of the filter values and the input values of the pixel (remember—pixel values range from 0 to 255, but they are normalized to keep the values between 0 to 1) to calculate a feature map, and we will take a step size (stride) of n=1 to the right."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is repeated until we reach the top-right corner of the image, and then we start over again from one down cell from the left of the image until we finish scanning the whole image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While convolving the filter, we calculate the dot product of the weights on the filter with the input values. For example, as shown in Figure 5.2, the input size of 5 by 5 (25 in total) is being scanned by a filter of size 9 (3 by 3), and the dot product of the filter values and the input values of the pixel is being calculated in the output array:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the case of a DNA sequence as an input, after creating a one-hot encoding (Figure 5.3), the convolutions are performed similar to image data. Each filter chosen will act as a sliding window."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_05_003.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next layer after the convolution layer is the pooling layer, which reduces the number of parameters, memory usage, and computational cost, and prevents overfitting in the network progressively (Figure 5.4). It is quite normal to have pooling layers between multiple convolutional layers. The pooling layer operates as a downsampling layer, and while doing so, computes either an average or a maximum value for a specified filter size and stride (slide is nothing but a sliding window size):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_05_004.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_05_006.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_05_007.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_05_008.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
