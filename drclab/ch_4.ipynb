{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recently, there has been a rapid increase in interest in genomics-based applications in the biomedical, pharmaceutical, and therapeutics industries. Machine learning (ML), with its sophisticated mathematical and data analysis techniques, coupled with advances in next-generation sequencing (NGS) have played a huge role in this rapid rise. As most genomic companies and other research organizations started to produce genomic data to keep themselves ahead of the curve, the ability to extract novel biological insights and build predictive models from this ever-growing data has proved to be a challenge for ML because it relied on hand-crafted features for model training and predictions as we saw in the previous two chapters. Translating this massive genomic data from an incomprehensible resource into meaningful insights automatically and intuitively requires more expressive ML models and algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_04_001.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_04_002.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_04_003.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function is the key component of a neural network as it introduces the non-linearity in the architecture. By definition, they transform the summed weighted input from the input node (the output from the transfer function) into an output value to be passed on to the next layer in the architecture (Figure 4.3). Activation functions are unique components of any DL architecture. They help convert a linear relationship into a nonlinear relationship, which is the key to solving so many of the problems that cannot be typically solved by ML. Without the activation function, the network would be just a linear combination of input values. Activation functions will decide if an input to the network is important and should be passed on further or not. This is where DL is different from traditional ML; for example, without the non-linear activation functions, which is the case with ML models, the neural networks behave just like a linear regression function. The activation function helps the DNNs keep the most useful information and filters all irrelevant data points.\n",
    "\n",
    "Since activation functions are very important, let’s spend some time understanding the different activation functions and where they can be applied. Several types of activation functions are available. These will be described in the following subsections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_04_004.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_04_005.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_04_006.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_04_007.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_04_008.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_04_009.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation is the method through which neural networks make predictions. This network uses multiple layers (input, hidden, and output) to make predictions. For example, in this simple network, a single pass of forwarding propagation looks similar to what’s shown in Figure 4.10:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_04_010.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation is the opposite of forward propagation and is the most common algorithm for neural networks. It is the process of propagating the errors back into the network to update the weights at each node in the network so that they cause the original output to be closer to the target output, thereby lowering the error overall (Figure 4.11). It works by calculating the loss in the output layer by comparing the predictions with the observed values. The derivative concerning the weight is then calculated using the chain rule and then updates the weights, as shown in Figure 4.11:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_04_011.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "One of the key concepts in DL is to avoid biases and variance in the model. Among them, overfitting is the most important one. Regularization is a set of strategies used in DL to reduce the overfitting of the model and improve model predictions. Most models perform well after being trained on a specific subset of data but often, they fail on real-world data, which means they fail to generalize well. Regularization strategies aim to address overfitting and keep the training error as low as possible.\n",
    "\n",
    "There are three types of regularization techniques. Let’s take a look.\n",
    "\n",
    "- Lasso\n",
    "In this method, the coefficients of the network are shrunk to 0 and because of that, it is suitable for variable selection.\n",
    "\n",
    "- Ridge\n",
    "In this method, the coefficients of the network shrink to smaller values (but not 0).\n",
    "\n",
    "- Elastic Net\n",
    "This method combines Lasso and Ridge and is a tradeoff between both methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
