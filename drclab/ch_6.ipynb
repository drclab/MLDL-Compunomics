{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The most important type of sequential data is the time series data, which is a series of data points listed in time order. This data is key for applications such as speech recognition, sentiment analysis, language translation, and so on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field of genomics, which consists of the most natural language ever – a sequence of nucleotides (A, G, C, and T) – is very well suited for RNNs applications, such as for predicting proteins from DNA sequences, predicting the binding domains of proteins, predicting the interaction between enhancers and promoters, predicting structural motifs, predicting base calls from sequencing instruments, optimizing coding sequences for increased protein production, predicting function, and so on. In this chapter, you will learn what RNNs are, how they are different from FNNs and CNNs, and how they are better suited for sequential data. By the end of this chapter, you will understand what RNNs are and why they are important in DL, the different types of RNN architectures and when to use what, and the different RNN applications in genomics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_06_001.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_06_002.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_06_003.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_06_004.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another good way of illustrating how RNNs work is to explain it with an example: Imagine you have a standard FNN and give it a DNA sequence (ATGCGAG) and it processes one nucleotide at a time but by the time it reaches the last nucleotide (in this example ‘G’) it has forgotten everything about other nucleotides ‘A’, ‘T’, ‘G’, ‘C’, ‘G’, ‘A’ and FNN can't predict what nucleotide would come next. This information is important for sequential data such as DNA sequences because there is a structure to the sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding RNNs through Transcription Factor Binding Site (TFBS) predictions Transcription factors (TF) play a key role in gene regulation, particularly during transcription, where they bind to the promoter regions and initiate the process of transcription. Transcription Factor Binding Sites (TFBSs) in DNA are short sequences in gene regulatory regions (such as promoters) and typically range in size from 5 bp to 20 bp. Each TF binds to a different TFBS and controls gene regulation in the cell. Thus, identifying the TF binding sites is key for us to understand cellular and molecular processes. Several experimental methods can identify TFBSs, such as ChIP-Seq technologies and databases such as ENCODE, which have made TFBS information available to researchers. However, ChIP-Seq technologies are expensive, slow, and laborious, and cannot find patterns in the identified TFBS. Several computational methods have become the go-to for solving this very important problem of identifying TFBSs. Given a particular sequence, predicting whether it is a TFBS or not is the core task of bioinformatics. In the following toy example, let’s see how we can use RNNs to predict a TFBS from DNA sequences. The problem of TFBS can be thought of as a binary classification problem – that is, whether the TFBS can be found in a DNA sequence or not, which we represent as 1 or 0, respectively. The input to the RNN model is the input DNA sequences and their targets, which have labels of 1 or 0. The goal here is to build a highly accurate classification model using an RNN that can be used to"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_06_015.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As shown in Figure 6.15, samples 1 to 3 consist of positive TFBSs (label=1), where the DNA sequences consist of TFBSs, whereas samples 4 and 5 are negative (label=0):\n",
    "\n",
    "- The first thing we must do is convert DNA sequence into a one-hot encoding vector. To refresh your memory, a one-hot encoding vector converts each nucleotide of the DNA sequence into a binary vector, labeled 0 or 1.\n",
    "- After the input is fed into the RNN, it produces another matrix. As we just learned, at each timestamp, the RNN takes an input vector and the previously hidden state vector and produces the new hidden state recursively. In this case, each position in the sequence is a timestamp. At the end of the training process, the RNN produces an output vector at the timestamp of the input sequence (Figure 6.16).\n",
    "- Then, the output vector from the RNN is fed into a softmax activation function in the last layer of the network, which learns the mapping between the hidden space and the target label (0 or 1). The final output is a probability that indicates whether the DNA sequence is a TFBS or a non-TFBS.\n",
    "Like other FNNs and RNNs, we calculate the loss (cross-entropy loss) and then the model is trained until the network generates low or no loss. This minimization of the loss function is achieved using the BPTT algorithm. We can use dropout as a regularization method for the model to prevent overfitting:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781804615447/files/image/B18958_06_016.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The human reference genome was first divided into 200 bp non-overlapping segments\n",
    "For each of the 690 ChIP-seq experiments, if 100 bp-200 bp segments belonged to a peak, it was classified as positive (label=1); otherwise, it was classified as negative (label=0)\n",
    "800 bp (400 bp on either side) sequences was addede to both sides of the 200 bp segment to create a 1,000 bp input sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After feeding input to the network through the input layer, the next layer will be a CNN. You might be wondering why we are using a CNN since the goal is to leverage an RNN for a genomics problem. This is because the CNN layer acts as a motif scanner, as we learned in the previous chapter.\n",
    "- The output from the CNN is fed into the BiLSTM layer. The output from the BiLSTM layer is then flattened and fed into a fully connected layer.\n",
    "- In the output layer of the network, a sigmoid function is applied. The final output is a 690-dimensional vector, where each element corresponds to the ChIP-seq experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('../Chapter06/data/X_train.npy.zip')['X_train']\n",
    "y_train = np.load('../Chapter06/data/y_train.npy.zip')['y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 690)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('../Chapter06/data/X_test.npy.zip')['X_test']\n",
    "y_test = np.load('../Chapter06/data/y_test.npy.zip')['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 690)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 10:48:33.357529: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-23 10:48:39.700361: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-23 10:48:39.700383: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-23 10:48:53.654372: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-23 10:48:53.654661: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-23 10:48:53.654679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Layer, Input\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(shape=(1000,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Conv1D(320, kernel_size=26, activation='relu')(input_data)\n",
    "output = MaxPooling1D()(output)\n",
    "output = Dropout(0.2)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Bidirectional(LSTM(320, return_sequences=True))(output)\n",
    "output = Dropout(0.5)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_output = Flatten()(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_output = Dense(695)(flat_output)\n",
    "FC_output = Activation('relu')(FC_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Dense(690)(FC_output)\n",
    "output = Activation('sigmoid')(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_data, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1000, 4)]         0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 975, 320)          33600     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 487, 320)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 487, 320)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 487, 640)         1640960   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 487, 640)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 311680)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 695)               216618295 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 695)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 690)               480240    \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 690)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 218,773,095\n",
      "Trainable params: 218,773,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = ModelCheckpoint(filepath='./model/bilstm_model.hdf5', verbose=1, save_best_only=False)\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0877\n",
      "Epoch 1: saving model to ./model/bilstm_model.hdf5\n",
      "90/90 [==============================] - 725s 8s/step - loss: 0.0877 - val_loss: 0.0603\n",
      "Epoch 2/2\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0601\n",
      "Epoch 2: saving model to ./model/bilstm_model.hdf5\n",
      "90/90 [==============================] - 677s 8s/step - loss: 0.0601 - val_loss: 0.0639\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=100, \n",
    "                    epochs=2, shuffle=True, verbose=1, validation_split=0.1, \n",
    "                    callbacks=[checkpoints,earlystopper])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RNNs are a special type of neural network that is well suited for sequential data such as time series, audio, video, and text. Research showed that RNNs have improved the performance of sequential data types when compared to other architectures such as FNNs and CNNs. The key to an RNN is the sequence memory state, which helps it store information from the previously analyzed state; this is good for sequential signal analysis and predictive analysis. In this chapter, we learned how RNNs are different from FNNs and CNNs. We understood the different types of RNNs and what makes them good for sequential data analysis by looking at a few examples. RNNs, as you may have noticed, are good for mapping a fixed or variable-sized input sequence to a fixed or variable-sized output; we have seen several examples to understand this.\n",
    "\n",
    "##### We also looked at how RNNs can help with genomics tasks and understood the different architectural types of RNNs. Bidirectional RNN, LSTM, and GRU are variants of RNNs that are capable of long-term associations, thereby retaining the information from an infinite sequence, which is very common in genomics. They address long-term dependencies.\n",
    "\n",
    "##### You were also introduced to the different RNN types and their applications in various domains, such as image captioning, language translation, and others. Finally, we looked at how RNNs are used to solve some of the key problems in genomics, such as TF binding site detection, miRNA-mRNA sequence modeling, gene expression analysis, histone modifications, base calling, and more. In the next chapter, we will look at another exciting neural network architecture called autoencoders, which has a lot of potential applications in genomics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
